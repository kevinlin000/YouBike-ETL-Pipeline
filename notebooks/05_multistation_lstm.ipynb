{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3c20a5-aa94-47b9-8e67-de46b587a98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "[Info] æˆåŠŸè®€å–è¨“ç·´è³‡æ–™ï¼Œå…± 4377102 ç­†\n",
      "[Info] æˆåŠŸè®€å–ç«™é»è³‡è¨Šæª”\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# æª¢æŸ¥è£ç½®\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- è¨­å®šè·¯å¾‘ ---\n",
    "data_path = '../data/processed/youbike_weather_merged.csv'\n",
    "info_path = '../data/raw/station_info.csv'\n",
    "\n",
    "# 1. è®€å–ä¸»è¨“ç·´è³‡æ–™\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['record_time'] = pd.to_datetime(df['record_time'])\n",
    "    print(f\"[Info] æˆåŠŸè®€å–è¨“ç·´è³‡æ–™ï¼Œå…± {len(df)} ç­†\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"æ‰¾ä¸åˆ°è¨“ç·´è³‡æ–™ï¼š{data_path}\")\n",
    "\n",
    "# 2. è®€å–ç«™é»è³‡è¨Š (ç”¨ä¾†æŸ¥ä¸­æ–‡ç«™å)\n",
    "if os.path.exists(info_path):\n",
    "    # é˜²å‘†ï¼šå˜—è©¦ç”¨ Tab åˆ†éš”è®€å–ï¼Œå¤±æ•—å‰‡ç”¨é€—è™Ÿ\n",
    "    try:\n",
    "        df_info = pd.read_csv(info_path, sep='\\t')\n",
    "        if len(df_info.columns) <= 1: df_info = pd.read_csv(info_path)\n",
    "    except:\n",
    "        df_info = pd.read_csv(info_path)\n",
    "        \n",
    "    # å»ºç«‹å°ç…§è¡¨ï¼šstation_no -> ä¸­æ–‡åç¨± (å»é™¤ YouBike2.0_ å‰ç¶´)\n",
    "    name_map = dict(zip(df_info['station_no'].astype(str), df_info['name_tw'].str.replace('YouBike2.0_', '')))\n",
    "    print(f\"[Info] æˆåŠŸè®€å–ç«™é»è³‡è¨Šæª”\")\n",
    "else:\n",
    "    print(f\"[Warning] æ‰¾ä¸åˆ°ç«™é»è³‡è¨Šæª”ï¼Œå°‡ä½¿ç”¨é è¨­åç¨±\")\n",
    "    name_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71bc5ffc-bba3-42c8-bb15-500be4c09ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ç‰¹å¾µå·¥ç¨‹å®Œæˆï¼šå·²æ–°å¢ 'Rain_Cat' (é™é›¨åˆ†ç´š)\n",
      "          record_time  rain  Rain_Cat\n",
      "0 2025-12-09 21:31:29   0.0         0\n",
      "1 2025-12-09 21:31:29   0.0         0\n",
      "2 2025-12-09 21:31:29   0.0         0\n",
      "3 2025-12-09 21:31:29   0.0         0\n",
      "4 2025-12-09 21:31:29   0.0         0\n"
     ]
    }
   ],
   "source": [
    "# --- å®šç¾©é™é›¨åˆ†ç´šå‡½æ•¸ ---\n",
    "def get_rain_category(val):\n",
    "    if val == 0: return 0      # ç„¡é›¨\n",
    "    elif val <= 2: return 1    # æ¯›æ¯›é›¨ (Drizzle)\n",
    "    elif val <= 10: return 2   # å°é›¨/ä¸­é›¨ (Rain)\n",
    "    else: return 3             # å¤§é›¨ (Heavy)\n",
    "\n",
    "# ç¢ºä¿ rain æ¬„ä½ç„¡ç©ºå€¼\n",
    "df['rain'] = df['rain'].fillna(0)\n",
    "\n",
    "# æ‡‰ç”¨å‡½æ•¸ç”¢ç”Ÿæ–°ç‰¹å¾µ\n",
    "df['Rain_Cat'] = df['rain'].apply(get_rain_category)\n",
    "\n",
    "print(\" ç‰¹å¾µå·¥ç¨‹å®Œæˆï¼šå·²æ–°å¢ 'Rain_Cat' (é™é›¨åˆ†ç´š)\")\n",
    "print(df[['record_time', 'rain', 'Rain_Cat']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9ce21cb-3fdc-428f-ac4d-5a1c66f0fbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç™¼ç¾è¡Œæ”¿å€: ['å¤§å®‰å€' 'å¤§åŒå€' 'å£«æ—å€' 'æ–‡å±±å€' 'ä¸­æ­£å€' 'ä¸­å±±å€' 'å…§æ¹–å€' 'åŒ—æŠ•å€' 'æ¾å±±å€' 'å—æ¸¯å€' 'ä¿¡ç¾©å€' 'è¬è¯å€'\n",
      " 'è‡ºå¤§å…¬é¤¨æ ¡å€']\n",
      " æœ€çµ‚é–å®š 13 å€‹åˆ†å€ä»£è¡¨ç«™é»é€²è¡Œè¨“ç·´\n",
      " é€™äº›ç«™é»å°‡æœƒå‡ºç¾åœ¨ Dashboard ä¸‹æ‹‰é¸å–®ä¸­ã€‚\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3. åŸ·è¡Œåˆ†å€é¸ç«™é‚è¼¯ (å®Œæ•´ 13 å€)\n",
    "# ==========================================\n",
    "selected_stations = []\n",
    "station_info_map = {} # Dashboard å°ˆç”¨å­—å…¸\n",
    "\n",
    "if 'district' in df.columns:\n",
    "    districts = df['district'].unique()\n",
    "    print(f\"ç™¼ç¾è¡Œæ”¿å€: {districts}\")\n",
    "    \n",
    "    for dist in districts:\n",
    "        # æ‰¾å‡ºè©²è¡Œæ”¿å€çš„æ‰€æœ‰è³‡æ–™\n",
    "        dist_df = df[df['district'] == dist]\n",
    "        \n",
    "        if not dist_df.empty:\n",
    "            # æ‰¾å‡ºè©²å€å‡ºç¾æ¬¡æ•¸æœ€å¤šè€… (æµé‡æœ€å¤§ä»£è¡¨ç«™)\n",
    "            top_station = dist_df['station_no'].value_counts().idxmax()\n",
    "            \n",
    "            # å¼·åˆ¶è½‰ç‚ºå­—ä¸²å„²å­˜ï¼Œé¿å… ID å‹æ…‹æ··äº‚\n",
    "            station_id_str = str(top_station)\n",
    "            selected_stations.append(station_id_str)\n",
    "            \n",
    "            # æŸ¥çœŸå¯¦ä¸­æ–‡åå­—\n",
    "            real_name = name_map.get(station_id_str, f\"{dist}ä»£è¡¨ç«™\")\n",
    "            \n",
    "            # çµ„åˆé¡¯ç¤ºåç¨±ï¼š \"æ·é‹ç§‘æŠ€å¤§æ¨“ç«™ (å¤§å®‰å€)\"\n",
    "            station_info_map[station_id_str] = f\"{real_name} ({dist})\"\n",
    "            \n",
    "    # é€™é‚Šæˆ‘å€‘ä¸å†åˆ‡å‰ 5 å€‹ï¼Œè€Œæ˜¯ä¿ç•™å…¨éƒ¨ (13å€‹)\n",
    "    all_target_stations = selected_stations\n",
    "\n",
    "else:\n",
    "    print(\"[Error] éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° 'district' æ¬„ä½\")\n",
    "    all_target_stations = []\n",
    "\n",
    "print(f\" æœ€çµ‚é–å®š {len(all_target_stations)} å€‹åˆ†å€ä»£è¡¨ç«™é»é€²è¡Œè¨“ç·´\")\n",
    "print(f\" é€™äº›ç«™é»å°‡æœƒå‡ºç¾åœ¨ Dashboard ä¸‹æ‹‰é¸å–®ä¸­ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "151ed7b4-0d3c-4c96-baf3-b4b2916d1aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Station Mapping Created: {'500101037': 0, '500103021': 1, '500104112': 2, '500105044': 3, '500106074': 4, '500107111': 5, '500108169': 6, '500109016': 7, '500110090': 8, '500111079': 9, '500112087': 10, '500113059': 11, '500119072': 12}\n",
      "ğŸ“Š è³‡æ–™éæ¿¾å®Œæˆï¼Œç¸½æ¨£æœ¬æ•¸: 33311\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. å»ºç«‹ Mapping èˆ‡ è£œå€¼ (é‡å° 13 ç«™)\n",
    "# ==========================================\n",
    "\n",
    "# 1. ç¯©é¸è³‡æ–™ (åªä¿ç•™é€™ 13 å€‹ä»£è¡¨ç«™)\n",
    "df_filtered = df[df['station_no'].astype(str).isin(all_target_stations)].copy()\n",
    "\n",
    "# 2. è£½ä½œç«™é» ID Mapping (0, 1, 2, ..., 12)\n",
    "# éµ (Key) å¿…é ˆæ˜¯å­—ä¸²ï¼Œé€™æ¨£ API æ¥æ”¶å‰ç«¯å‚³ä¾†çš„å­—ä¸² ID æ‰æ‰¾å¾—åˆ°\n",
    "station_mapping = {str(station): idx for idx, station in enumerate(all_target_stations)}\n",
    "df_filtered['station_idx'] = df_filtered['station_no'].astype(str).map(station_mapping)\n",
    "\n",
    "print(\"âœ… Station Mapping Created:\", station_mapping)\n",
    "\n",
    "# 3. åŸ·è¡Œè£œå€¼ (åŒ…å«æ–°çš„ Rain_Cat)\n",
    "features_to_fill = ['bikes_available', 'temperature', 'rain', 'Rain_Cat']\n",
    "\n",
    "# å…ˆæ’åºï¼Œç¢ºä¿æ™‚é–“åºåˆ—æ­£ç¢º\n",
    "df_filtered = df_filtered.sort_values(['station_idx', 'record_time'])\n",
    "\n",
    "# é‡å°æ¯å€‹ç«™é»ç¨ç«‹é€²è¡Œè£œå€¼ (ç·šæ€§æ’å€¼ + å‰å¾Œå¡«è£œ)\n",
    "df_filtered[features_to_fill] = df_filtered.groupby('station_idx')[features_to_fill].transform(\n",
    "    lambda x: x.interpolate(method='linear').ffill().bfill()\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š è³‡æ–™éæ¿¾å®Œæˆï¼Œç¸½æ¨£æœ¬æ•¸: {len(df_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b652760-6788-48af-95f2-803a6a7f609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " åºåˆ—è³‡æ–™è£½ä½œå®Œæˆã€‚Input Shape: torch.Size([33272, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# è¨­å®šç‰¹å¾µæ¬„ä½ (å…± 4 å€‹)\n",
    "feature_cols = ['bikes_available', 'temperature', 'rain', 'Rain_Cat']\n",
    "\n",
    "# 1. æ•¸æ“šç¸®æ”¾ (Scaler)\n",
    "scaler = MinMaxScaler()\n",
    "df_filtered[feature_cols] = scaler.fit_transform(df_filtered[feature_cols])\n",
    "\n",
    "# 2. è£½ä½œæ»‘å‹•è¦–çª— (Sliding Window)\n",
    "def create_multistation_dataset(data, time_steps=3):\n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    for station_idx in data['station_idx'].unique():\n",
    "        station_data = data[data['station_idx'] == station_idx]\n",
    "        \n",
    "        values = station_data[feature_cols].values # æ•¸å€¼ç‰¹å¾µ\n",
    "        ids = station_data['station_idx'].values   # ç«™é» ID\n",
    "        \n",
    "        for i in range(len(values) - time_steps):\n",
    "            # å–éå» 3 å°æ™‚çš„ç‰¹å¾µ\n",
    "            seq_values = values[i:i+time_steps]\n",
    "            # å–å°æ‡‰çš„ ID (é‡å¡‘å½¢ç‹€ä»¥åˆ©æ‹¼æ¥)\n",
    "            seq_ids = ids[i:i+time_steps].reshape(-1, 1)\n",
    "            \n",
    "            # åˆä½µï¼š[ç‰¹å¾µ(4) + ID(1)] = 5 å€‹æ¬„ä½\n",
    "            combined_input = np.hstack((seq_values, seq_ids))\n",
    "            \n",
    "            # Label: ä¸‹ä¸€æ™‚åˆ»çš„ bikes_available\n",
    "            target = values[i + time_steps, 0]\n",
    "            \n",
    "            X_list.append(combined_input)\n",
    "            y_list.append(target)\n",
    "            \n",
    "    return np.array(X_list), np.array(y_list)\n",
    "\n",
    "# åŸ·è¡Œè½‰æ›\n",
    "TIME_STEPS = 3\n",
    "X, y = create_multistation_dataset(df_filtered, TIME_STEPS)\n",
    "\n",
    "# è½‰æˆ PyTorch Tensor ä¸¦ä¸Ÿé€² GPU/CPU\n",
    "X_tensor = torch.FloatTensor(X).to(device)\n",
    "y_tensor = torch.FloatTensor(y).reshape(-1, 1).to(device)\n",
    "\n",
    "print(f\" åºåˆ—è³‡æ–™è£½ä½œå®Œæˆã€‚Input Shape: {X_tensor.shape}\")\n",
    "# é æœŸçµæœ: (æ¨£æœ¬æ•¸, 3, 5) -> 3æ˜¯æ™‚é–“æ­¥, 5æ˜¯ç‰¹å¾µæ•¸(4æ•¸å€¼+1ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b5d3419-2675-47ff-9441-b3c216627925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " æ¨¡å‹æ¶æ§‹å®šç¾©å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "class MultiStationLSTM(nn.Module):\n",
    "    def __init__(self, num_stations, input_size=4, hidden_size=64, output_size=1, embedding_dim=5):\n",
    "        super(MultiStationLSTM, self).__init__()\n",
    "        \n",
    "        # 1. ç«™é»åµŒå…¥å±¤ (æŠŠ 0~4 çš„ ID è½‰æˆå‘é‡)\n",
    "        self.station_embedding = nn.Embedding(num_stations, embedding_dim)\n",
    "        \n",
    "        # 2. LSTM å±¤\n",
    "        # è¼¸å…¥ç¶­åº¦ = æ•¸å€¼ç‰¹å¾µ(4) + Embedding(5) = 9\n",
    "        self.lstm_input_size = input_size + embedding_dim\n",
    "        self.lstm = nn.LSTM(self.lstm_input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # 3. è¼¸å‡ºå±¤\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, time, 5) \n",
    "        # åˆ‡åˆ†è³‡æ–™ï¼šå‰4å€‹æ˜¯æ•¸å€¼ï¼Œæœ€å¾Œ1å€‹æ˜¯ID\n",
    "        numerical_features = x[:, :, :4] \n",
    "        station_ids = x[:, :, 4].long()\n",
    "        \n",
    "        # åµŒå…¥é‹ç®—\n",
    "        station_embedded = self.station_embedding(station_ids)\n",
    "        \n",
    "        # æ‹¼æ¥ (4 + 5 = 9)\n",
    "        combined_input = torch.cat((numerical_features, station_embedded), dim=2)\n",
    "        \n",
    "        # LSTM é‹ç®—\n",
    "        out, _ = self.lstm(combined_input)\n",
    "        \n",
    "        # å–æœ€å¾Œä¸€å€‹æ™‚é–“é»çš„è¼¸å‡º\n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "print(\" æ¨¡å‹æ¶æ§‹å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e68701d-2a39-4fc1-ab43-78c537c2313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " é–‹å§‹è¨“ç·´æ¨¡å‹...\n",
      "Epoch [10/100], Loss: 0.0623\n",
      "Epoch [20/100], Loss: 0.0450\n",
      "Epoch [30/100], Loss: 0.0400\n",
      "Epoch [40/100], Loss: 0.0346\n",
      "Epoch [50/100], Loss: 0.0293\n",
      "Epoch [60/100], Loss: 0.0237\n",
      "Epoch [70/100], Loss: 0.0177\n",
      "Epoch [80/100], Loss: 0.0123\n",
      "Epoch [90/100], Loss: 0.0086\n",
      "Epoch [100/100], Loss: 0.0071\n",
      " è¨“ç·´çµæŸï¼\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "num_stations = len(station_mapping)\n",
    "model = MultiStationLSTM(num_stations=num_stations, input_size=4).to(device)\n",
    "\n",
    "# è¨­å®šè¶…åƒæ•¸\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\" é–‹å§‹è¨“ç·´æ¨¡å‹...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\" è¨“ç·´çµæŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4c53e95-071b-41df-a8e8-e5f493f00402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " è¨“ç·´èˆ‡å­˜æª”å®Œæˆï¼\n",
      "ç¾åœ¨æ¨¡å‹å·²æ”¯æ´ä»¥ä¸‹ 13 å€‹ç«™é»çš„é æ¸¬ï¼š\n",
      " - 500101037: è‡ºå¤§åœ‹é’å¤§æ¨“å®¿èˆå‰ (å¤§å®‰å€)\n",
      " - 500103021: å¤§ç¨»åŸ•å…¬åœ’ (å¤§åŒå€)\n",
      " - 500104112: åŠæ½­éƒµå±€ (å£«æ—å€)\n",
      " - 500105044: ç§€æ˜è·¯ä¸€æ®µ185å··å£ (æ–‡å±±å€)\n",
      " - 500106074: ç‹è²«è‹±å…ˆç”Ÿç´€å¿µåœ–æ›¸é¤¨ (ä¸­æ­£å€)\n",
      " - 500107111: é•·æ˜¥é¾æ±Ÿè·¯å£ (ä¸­å±±å€)\n",
      " - 500108169: å¨å‰›ç§‘æŠ€ç¸½éƒ¨ (å…§æ¹–å€)\n",
      " - 500109016: æ·é‹å¾©èˆˆå´—ç«™æ—…å®¢æœå‹™ä¸­å¿ƒ (åŒ—æŠ•å€)\n",
      " - 500110090: å¥åº·å…¬åœ’ (æ¾å±±å€)\n",
      " - 500111079: æ·é‹æ˜†é™½ç«™(4è™Ÿå‡ºå£) (å—æ¸¯å€)\n",
      " - 500112087: æ¾ä»è·¯305è™Ÿ (ä¿¡ç¾©å€)\n",
      " - 500113059: é•·é †è‰‹èˆºå¤§é“å£ (è¬è¯å€)\n",
      " - 500119072: è‡ºå¤§åŸåˆ†æ‰€åŒ—å´ (è‡ºå¤§å…¬é¤¨æ ¡å€)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 8.  å­˜æª”å€ (æ¨¡å‹èˆ‡å°ç…§è¡¨åŒæ­¥)\n",
    "# ==========================================\n",
    "import joblib\n",
    "\n",
    "# ç¢ºä¿å„²å­˜ç›®éŒ„å­˜åœ¨\n",
    "save_path = '../api/model_files'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# 1. å„²å­˜æ¨¡å‹æ¬Šé‡ (.pth)\n",
    "torch.save(model.state_dict(), os.path.join(save_path, 'youbike_lstm_multistation.pth'))\n",
    "\n",
    "# 2. å„²å­˜æ•¸å€¼ç¸®æ”¾å™¨ (.pkl)\n",
    "joblib.dump(scaler, os.path.join(save_path, 'scaler.pkl'))\n",
    "\n",
    "# 3. å„²å­˜ç«™é»ç´¢å¼•å°ç…§è¡¨ (Key æ˜¯å­—ä¸²ï¼Œé€™é»å° API è‡³é—œé‡è¦)\n",
    "joblib.dump(station_mapping, os.path.join(save_path, 'station_mapping.pkl'))\n",
    "\n",
    "# 4. å„²å­˜ Dashboard å°ˆç”¨å­—å…¸ (åªä¿ç•™æœ‰åƒèˆ‡è¨“ç·´çš„ç«™é»)\n",
    "final_dashboard_map = {sid: name for sid, name in station_info_map.items() if sid in station_mapping}\n",
    "joblib.dump(final_dashboard_map, os.path.join(save_path, 'station_info_map.pkl'))\n",
    "\n",
    "print(\" è¨“ç·´èˆ‡å­˜æª”å®Œæˆï¼\")\n",
    "print(f\"ç¾åœ¨æ¨¡å‹å·²æ”¯æ´ä»¥ä¸‹ {len(station_mapping)} å€‹ç«™é»çš„é æ¸¬ï¼š\")\n",
    "for sid, name in final_dashboard_map.items():\n",
    "    print(f\" - {sid}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d3405a-78ef-42cd-afb7-9b7dd563998d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (PyTorch)",
   "language": "python",
   "name": "youbike_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
