{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6246ccdc-3b54-4737-815b-c2c20446eef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å•Ÿå‹• Tableau å°ˆç”¨å¯¬è¡¨åˆæˆç¨‹åº...\n",
      "ğŸ“– æˆåŠŸè®€å–: youbike_weather_merged.csv (utf-8, é€—è™Ÿ)\n",
      "ğŸ“– æˆåŠŸè®€å–: station_info.csv (utf-8, Tab)\n",
      "ğŸ”— æ­£åœ¨åŸ·è¡Œæ•¸æ“šé—œè¯ (å·¦è¡¨æ¬„ä½: 15, å³è¡¨æ¬„ä½: 4)...\n",
      "------------------------------\n",
      "âœ¨ çµ‚æ–¼æˆåŠŸäº†ï¼\n",
      "ğŸ“Š æœ€çµ‚å¤§è¡¨åŒ…å« 18 å€‹æ¬„ä½\n",
      "ğŸ“ æª”æ¡ˆè·¯å¾‘: /Users/kevinlintingwei/YouBike-ETL-Pipeline/data/tableau_master_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "root_path = \"/Users/kevinlintingwei/YouBike-ETL-Pipeline/data\"\n",
    "processed_path = os.path.join(root_path, \"processed\")\n",
    "raw_path = os.path.join(root_path, \"raw\")\n",
    "\n",
    "def smart_read_csv(file_path):\n",
    "    encodings = ['utf-8', 'big5', 'cp950', 'utf-8-sig']\n",
    "    separators = [',', '\\t']\n",
    "    for enc in encodings:\n",
    "        for sep in separators:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=enc, sep=sep)\n",
    "                # æ ¸å¿ƒä¿®æ­£ 1ï¼šç«‹å³åˆªé™¤é‡è¤‡çš„æ¬„ä½åç¨±ï¼ˆæ¨™é¡Œåˆ—å±¤ç´šï¼‰\n",
    "                df = df.loc[:, ~df.columns.duplicated()]\n",
    "                df.columns = df.columns.str.strip()\n",
    "                if len(df.columns) > 1:\n",
    "                    print(f\" æˆåŠŸè®€å–: {os.path.basename(file_path)} ({enc}, {'Tab' if sep=='\\t' else 'é€—è™Ÿ'})\")\n",
    "                    return df\n",
    "            except:\n",
    "                continue\n",
    "    raise Exception(f\" ç„¡æ³•è®€å–æª”æ¡ˆ {file_path}\")\n",
    "\n",
    "def create_tableau_master():\n",
    "    print(\" å•Ÿå‹• Tableau å°ˆç”¨å¯¬è¡¨åˆæˆç¨‹åº...\")\n",
    "    try:\n",
    "        df_merged = smart_read_csv(os.path.join(processed_path, \"youbike_weather_merged.csv\"))\n",
    "        df_info = smart_read_csv(os.path.join(raw_path, \"station_info.csv\"))\n",
    "        \n",
    "        # æ ¸å¿ƒä¿®æ­£ 2ï¼šçµ±ä¸€æ›´åï¼Œä½†é¿å…ç”¢ç”Ÿé‡è¤‡\n",
    "        rename_dict = {'ç«™é»ç·¨è™Ÿ': 'station_no', 'station_id': 'station_no', 'id': 'station_no'}\n",
    "        \n",
    "        for df in [df_merged, df_info]:\n",
    "            for old_col, new_col in rename_dict.items():\n",
    "                if old_col in df.columns and new_col not in df.columns:\n",
    "                    df.rename(columns={old_col: new_col}, inplace=True)\n",
    "            # ç¢ºä¿æ›´åå¾Œä¹Ÿæ²’æœ‰é‡è¤‡æ¨™ç±¤\n",
    "            df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "        # æ ¸å¿ƒä¿®æ­£ 3ï¼šç²¾ç°¡ info è¡¨ï¼Œåªå–åœ°åœ–å’Œåˆ†æéœ€è¦çš„æ¬„ä½\n",
    "        # é€™æ¨£å¯ä»¥é¿å… df_merged å’Œ df_info æœ‰å…¶ä»–åŒåæ¬„ä½å°è‡´ merge å¤±æ•—\n",
    "        keep_cols = ['station_no', 'station_name', 'lat', 'lng', 'district', 'station_name_en']\n",
    "        # åªä¿ç•™å­˜åœ¨çš„æ¬„ä½\n",
    "        available_cols = [c for c in keep_cols if c in df_info.columns]\n",
    "        df_info_clean = df_info[available_cols].drop_duplicates(subset=['station_no'])\n",
    "\n",
    "        # ç¢ºä¿é—œéµå­—ä¸²åŒ–\n",
    "        df_merged['station_no'] = df_merged['station_no'].astype(str)\n",
    "        df_info_clean['station_no'] = df_info_clean['station_no'].astype(str)\n",
    "        \n",
    "        print(f\"ğŸ”— æ­£åœ¨åŸ·è¡Œæ•¸æ“šé—œè¯ (å·¦è¡¨æ¬„ä½: {len(df_merged.columns)}, å³è¡¨æ¬„ä½: {len(df_info_clean.columns)})...\")\n",
    "        \n",
    "        # åŸ·è¡Œåˆä½µ\n",
    "        master_df = pd.merge(df_merged, df_info_clean, on='station_no', how='left')\n",
    "        \n",
    "        # è½‰æ›æ™‚é–“\n",
    "        master_df['record_time'] = pd.to_datetime(master_df['record_time'])\n",
    "        \n",
    "        # è¼¸å‡º\n",
    "        output_file = os.path.join(root_path, \"tableau_master_dataset.csv\")\n",
    "        master_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(\"-\" * 30)\n",
    "        print(f\" æœ€çµ‚å¤§è¡¨åŒ…å« {len(master_df.columns)} å€‹æ¬„ä½\")\n",
    "        print(f\" æª”æ¡ˆè·¯å¾‘: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\" ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        # traceback.print_exc() # å¦‚æœé‚„æ˜¯å¤±æ•—ï¼Œå–æ¶ˆé€™è¡Œè¨»è§£å¯ä»¥çœ‹åˆ°è©³ç´°å“ªä¸€è¡Œå ±éŒ¯\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_tableau_master()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42fafdc-58fb-47eb-b415-1474d0a4d7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
